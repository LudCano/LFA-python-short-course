{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Python short course on Atmospheric Data Analysis - Week 2  \n",
    "\n",
    "This Python tutorial was written in June 2024 by Ludving Cano, Research Assistant at the [Laboratory for Atmospheric Physics](http://www.chacaltaya.edu.bo) - UMSA (lcano@chacaltaya.edu.bo). It shows the basic I/O commands, opening dataset and start formatting data into Pandas dataframes.\n",
    "\n",
    "On **week 2** we will cover:\n",
    "\n",
    " - Usage of Paths in Python\n",
    " - Open and read files\n",
    " - Formatting data\n",
    " - `Pandas I`\n",
    "   - Opening a simple dataset\n",
    "   - Basic commands with pandas\n",
    "   - Opening a more complicated dataset\n",
    "   - Basic statistics with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries that we will use\n",
    "\n",
    "For this week we will start using libraries, depending on what do you want to do it's better to use or another. At the moment we will use:\n",
    "\n",
    " - [OS](https://docs.python.org/3/library/os.html) is a library used for general things that involve the system itself (your computer), we will use it mostly for opening and searching for files.\n",
    " - [Pathlib](https://docs.python.org/3/library/pathlib.html) is a library that offers some advantages while working with files, we will learn some things in parallel in OS.\n",
    " - [Pandas](https://pandas.pydata.org/) it's going to be our main library from now on, it offers A LOT of advantages when working with tables (or what we will call now, _dataframes_).\n",
    " - [Numpy](https://numpy.org/) it's mostly a numerical library, it offers advantages with numerical processing and calculations.\n",
    " \n",
    "I added a link to its main documentation in each bullet. Just to a click on it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Paths\n",
    "What's a path? It a route (in spanish _ruta_), it can tell us where a file is located and from there we can try to reach it. Paths are a combination of directories and subdirectories (and at the end the file itself) and they are usually separated by a backslash (\\\\) or forward slash (/).\n",
    "\n",
    "There are two main types: \n",
    "\n",
    " - Absolute path: It's where a file is located from the ROOT (or the origin) of the computer, the advantage is that even if we are accessing from a different directory each time, we can get the file path, the disadvantage is that if we change of computer we have to change our absolute path.\n",
    " - Relative path: It's where a file is located from where YOU are right now, for example, if we want to access any of our data we simply go to `data/`, the disadvantange is that if we change of directory from where we are executing our code, we have to change the relative path.\n",
    " \n",
    "\n",
    "If you want to see it this way, the absolute path is the current directory + the relative path.\n",
    "\n",
    "For example if you are on Jupyter lab you can generate the absolute path by doing a right click on a file, and from VSCode you have the option for both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color=\"green\" size=5>Example 1: Where are you from?</font></b>\n",
    "\n",
    "\n",
    "As said before, normally from where you are executing your code (a Python script for example) is called the  _current working directory_. I'll show how to show it in two ways, with Pathlib and OS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's not forget to import our libraries\n",
    "import os\n",
    "from pathlib import Path #--We will only use Path, not the entire library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, they practically show the same thing, it's where you are running your code from. The unique thing that changes is that when we use Path, we get an object called PosixPath (or this can change in Windows).\n",
    "\n",
    "Anyway, for the first method, `os.getcwd()`, we can see that is a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for showing which type of variable is os.getcwd()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing simple paths\n",
    "As you can see, we can generate to whatever we want by just defining a string with its content. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = 'data_samples/test1.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowing if something exists `os.path.exists()`\n",
    "Sometimes we want to know if a certain file exists or not (this is useful in cases when you work with well-formatted files, for example when the date is the name of the file). Then we want to know whether we can open this or not. If we try to open something that doesn't exist, the code will raise an error, so before opening it it's good to know this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "does_exist = os.path.exists(path_test)\n",
    "does_exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we will stay here, and if needed we will learn more things for `os` and `path`. Let's open our first file!\n",
    "\n",
    "# 2. Opening files\n",
    "\n",
    "## Opening a log data\n",
    "We will start with a _well behaved_ data, data from the [DECADE](https://www.geography.unibe.ch/research/climatology_group/research_projects/decade/index_eng.html), it's useful for meteorology experiments.\n",
    "\n",
    "Let's say the metadata for our dataset is stored in a file named `log_1_decade.txt` inside the `data_samples` directory. To open it let's simply use open and print our result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data_samples/log_1_decade.txt', 'r')\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened? We don't have the contents of our data. We only *opened* the file, now we need to *read* it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the content **as one single string**, this can be useful sometimes, sometimes we want to read line by line, for this we use the `readlines()` method (Note: we need to open the file again!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data_samples/log_1_decade.txt', 'r')\n",
    "lst = f.readlines()\n",
    "lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we got a list, where each element is a line of the file, it ends with a \\n which means a line break, to get rid of these we can do ([using list comprehension](https://www.w3schools.com/python/python_lists_comprehension.asp)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.strip() for i in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst2 = []\n",
    "for i in lst:\n",
    "    lst2.append(i.strip())\n",
    "lst2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we will need to get, for example, the altitude for each dataset (it's located in the 7th line), so, to get only the altitude we will do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line7 = lst2[6] #the 7th element is in the position 6, as it starts from 0\n",
    "altitude = int(line7.split(\":\")[1].strip()) #we split it by the colon :, this returns a list of the first and the second part and we care of the second one\n",
    "altitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening a pandas dataframe\n",
    "\n",
    "And... that's it for now, it may be hard (and it is), we will come back on using that dataset soon, for now let's open a _cute_ dataset, this contains the Ultraviolet Radiation taken with a sensor at the roof of our building for the 18th of June. The file is named `IUV_18_06.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most simple way to open a dataset is with the `read_csv()` function, as the first parameter we put the path to the file, the separator by default is comma (that's why we use the \\_csv function), as a result we get a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_samples/IUV_18_06.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing n first values\n",
    "To do this, we use the `df.head(n)` method, where n in an integer of how many rows we want to show: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Life sometimes is easy as that, we can know which type of data we have stored in each column with df.dtypes (Note: this is a method of the object df, the table itself has the property)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll learn how to call one specific column, for example the IUV column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.IUV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['IUV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some maths! we can get the maximum value in the IUV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.IUV.max() #which can be the min?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or simply, if we want to know the general info about a numerical column, we can describe it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.IUV.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big spoiler! We can plot within the column itself, we won't use this as can be difficult to customize, but as a first approach is fine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.IUV.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter things based on conditions, this has the format `df[condition]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.IUV > 0] #from when we have a measurement of UV?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting a dataset\n",
    "We can sort a dataset based on one or more columns, for example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('IUV', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will stop here (for now) with this dataset, now we will open a more complicated one, but be sure that this dataset will be useful for us.\n",
    "\n",
    "## Opening with headers and whitespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple version of a DECADE dataset is stored in `EL_ALTO_AEROPUERTO_simple.dat`. Let's inpect the first 20 rows to see how is the header and the first rows of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = open('data_samples/EL_ALTO_AEROPUERTO_simple.dat', 'r')\n",
    "[i.strip() for i in f2.readlines()[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see a header that takes 12 lines (including the --) that we will ignore. If we try to open it as it is we will get an error, as the read_csv interprets the first row as a header.\n",
    "\n",
    "Second problem, the data is separated by irregular intervals of whitespaces, there are fixes for these problems that are available in the [documentation of read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html). Let's do a break to read it!\n",
    "\n",
    "After reading the documentation, the way to read this dataset will be the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('data_samples/EL_ALTO_AEROPUERTO_simple.dat', delim_whitespace=True, skiprows=12)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have too many columns, we can't see it! We can get the names of columns using the method df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing columns and rows\n",
    "What is we only want the first 5 columns? Pandas offers two ways to slice a dataset `loc` and `iloc`. The former uses the labels and the latter the indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.iloc[:,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.loc[:,['Julday','Year','Mo','Da','Param=1']]\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of calling some columns is with double square brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[['Julday','Year','Mo','Da']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to rename ALL the columns? We can reassign all the column names using `df.columns = [newlist]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.columns = ['J','year','day','month','prcp']\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or if we want to only rename ONE (or not all) columns, we have to use a dictionary within the method df.rename.\n",
    "\n",
    "A dictionary is used by `dct = {'key':'val','key2':'val2'}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.rename(columns = {'J':'julday'}, inplace = True)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering with multiple conditions\n",
    "Sometimes we want to get slice by more than one filter, this can be done in a cascade (first apply one filter, then another), or we can apply both at the same time.\n",
    "\n",
    "For example, we want to know the maximum value of december for each year starting from 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = df3[(df3.year >= 2000) & (df3.month == 12)]\n",
    "dff.prcp.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This applies an AND for both conditions (it has to satisfy both of them), if we want to use an OR, we use |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color=\"green\" size=5>Exercise 1: It's rainy, isn't it? (Using df3)</font></b>\n",
    "\n",
    "1. Describe the dataset, which variables have which type of data.\n",
    "2. Store an slice named df4 from the year that you were born\n",
    "3. Get the maximum value of prcp in that year\n",
    "4. Sort df4 (in a descending order) and show only the first 10 values\n",
    "5. Which frequency does the data have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you'll probably need more than 1 cell, create as much as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b><font color=\"green\">Click here for the solution to Exercise 1</font></b></summary>\n",
    "\n",
    "```\n",
    "# Types of data\n",
    "print(df3.dtypes)\n",
    "\n",
    "# Getting slice of data\n",
    "df4 = df3[df3.year == 1999]\n",
    "\n",
    "# Get the max prcp\n",
    "max_prcp = df4.prcp.max()\n",
    "print(max_prcp)\n",
    "\n",
    "# Showing the 10 most rainy days\n",
    "df4.sort_values('prcp', ascending=False).head(10)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our first campaign dataset: Particulate matter (PM)\n",
    "This dataset was taken with an Airnote instrument (you'll get a talk about instrumentation later on), it's located in the file `data_samples/Airnote_forno.csv`.\n",
    "\n",
    "[Particulate matter (or PM)](https://www.epa.gov/pm-pollution/particulate-matter-pm-basics) in general are particles that lie in the air, during the campaign we take measurements of these to correlate the transport of contamination in general. Normally PM have categories depending on size, PM10, PM2.5, PM1, that are particulates of 10, 2.5 and 1 micrometers ($\\mu m$) and smaller, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"figs/memes/air_handler.png\" width=\"200\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color=\"green\" size=5>Exercise 2: PM stands for Papá and Mamá</font></b>\n",
    "\n",
    "1. Open the dataset described before\n",
    "2. Describe the columns related to PM\n",
    "3. What is the frequency of the data?\n",
    "4. Make a simple plot for PM10\n",
    "5. Make the rank of the highest measurements of PMX (pick one size)\n",
    "\n",
    "You'll probably need more than one cell, create as much as needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b><font color=\"green\">Click here for the solution to Exercise 2</font></b></summary>\n",
    "\n",
    "```\n",
    "#declaring the path\n",
    "pth = 'data_samples/Airnote_forno.csv'\n",
    "\n",
    "#reading the dataset\n",
    "dfpm = pd.read_csv(pth)\n",
    "\n",
    "#describing the relevant columns\n",
    "dfpm[['PM1','PM10','PM2.5']].describe()\n",
    "\n",
    "#plotting\n",
    "dfpm['PM10'].plot()\n",
    "\n",
    "#biggest measurements of PM10\n",
    "dfpm.sort_values('PM10', ascending=False).head(10)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the plot is not very helpful, as we cannot see at which time those peaks occur. We'll deal with time next week!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second dataset: Black Carbon (BC)\n",
    "Our next dataset comes from another instrument, the AE51 measures [Black Carbon](https://www.ccacoalition.org/short-lived-climate-pollutants/black-carbon), another way of contamination. The filepath is `data_samples/AE51_escuela_surface_300seconds.dat`\n",
    "\n",
    "If you open this dataset with a notepad, we can see that not only has a header, but it has a blank line between the column names and the data itself, this is a nono for our simple method. Luckily reading the documentation there is a parameter names `skip_blank_lines`, which ignore this row. Take note on the separator itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color=\"green\" size=5>Exercise 3: Black Carbon</font></b>\n",
    "\n",
    "1. Open the dataset described before\n",
    " - Take note on the separator\n",
    " - Use `skip_blank_lines = True`\n",
    "2. Which is the frecuency of the data?\n",
    "3. Get only the columns for time and BC\n",
    "4. Filter out negative values, as can be noise\n",
    "5. Do the rankings of biggest measurements again\n",
    "6. Plot BC\n",
    "\n",
    "You'll probably need more than one cell, create as much as needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b><font color=\"green\">Click here for the solution to Exercise 3</font></b></summary>\n",
    "\n",
    "```\n",
    "\n",
    "# Opening the dataset\n",
    "pth2 = 'data_samples/AE51_escuela_surface_300seconds.dat'\n",
    "dfae = pd.read_csv(pth2, skiprows = 15, skip_blank_lines=True, sep = ';')\n",
    "\n",
    "# Get the columns for date-time-BC\n",
    "dfae = dfae[['Date','Time','BC']]\n",
    "\n",
    "# Filter out negative values of BC\n",
    "dfae = dfae[dfae.BC >= 0]\n",
    "\n",
    "# Doing rankings again\n",
    "dfae.sort_values('BC', ascending=False).head(10)\n",
    "\n",
    "# Plotting\n",
    "dfae.BC.plot()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
